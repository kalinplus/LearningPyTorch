# 完整训练流程
1. 下载数据集，准备 dataset 和 dataloader
2. 下载/定义模型架构
3. 定义损失函数，优化器
4. 定义超参数
5. 开始训练（一个一个 epoch）
	1. 设置模型模式为 train
	2. 进入 dataloader 按 batch 读取数据用于训练的循环
	3. 优化器梯度清零
	4. 模型前向传播，计算损失
	5. 损失后向传播，优化器更新参数
	6. 打印日志，比如损失、准确率等指标
6. 开始验证
	1. 设置模型模式为 eval，torch.no_grad
	2. 进入 dataloader 按 batch 读取数据用于验证的循环
	3. 模型前向传播，计算损失
	4. 打印日志，比如损失、准确率等指标
	5. 根据指标，保存模型

在搭建整个流程中，推荐先搭建基础框架，再逐步添加功能。比如，我在写训练和验证的流程时，按照的顺序是：

开始训练！先搭建基本框架，再逐步添加更多功能  
1. 打印 loss。看起来没问题，能 train
2. 计算并打印训练阶段预测准确率  
3. 打印 1 个 epoch 的平均损失和准确率
4. 添加验证流程，并支持上述功能 1 2 3
5. 在 tensorboard 绘制 训练 和 验证 曲线  
6. 保存模型。做到如果在验证集上，准确率超过最优模型，则保存

Tips：网络训练通常需要第一个维度是 batch_size，如果维度不对/只是用一张图片测试，记得 `reshape` 

其实上述流程也包含了模型的验证/测试流程（验证和测试基本相同，只是可能要参数指定是测试才测试，指定训练则是训练+验证）

# 在GPU上训练
有两种方式。不过，都要首先找出代码中所有的模型和输入模型的数据。然后，
1. 对它们使用 `model = model.cuda()` （但是要确保 `torch.cuda.is_available() == true`, 否则报错）
2. 设定 `device = torch.device("cuda" if torch.cuda.is_available() else "cpu")` (用 `cuda: 0` 等可以指定多卡时用哪一张卡)，然后对所有这些使用 `model = model.to(device)`

推荐是用 第二种 方法，这样如果要换设备，只要改一处代码。

在大多数情况下，损失函数不需要显式地转移到 GPU，只要模型和数据已经正确地转移到 GPU，损失函数的计算会自动在 GPU 上进行。

# 看开源项目
老生常谈了，先读 README，下载下来先看需要哪些参数。如果有`required=True`的，可以考虑指定一个默认值，方便运行（或者 PyCharm 中编辑运行配置）